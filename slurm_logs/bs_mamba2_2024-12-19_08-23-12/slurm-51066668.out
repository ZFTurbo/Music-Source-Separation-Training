Running training script for model: bs_mamba2
training without optuna now
Exception during load Mamba2 modules: No module named 'mamba_ssm'
Load local torch implementation!
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 23, 23, 23, 23, 23, 23, 23, 23, 46, 46, 46, 46, 46, 46, 46, 46, 92, 92, 121]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 23, 23, 23, 23, 23, 23, 23, 23, 46, 46, 46, 46, 46, 46, 46, 46, 92, 92, 121]
Instruments: ['drums', 'bass', 'other', 'vocals']
Metrics for training: ['sdr']. Metric for scheduler: sdr
Use augmentation for training
Dataset type: 1 Processes to use: 8 
Collecting metadata for ['../data/MUSDB18HQ/train']
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:18<25:52, 18.26s/it]  2%|▏         | 2/86 [00:28<19:04, 13.62s/it] 10%|█         | 9/86 [00:42<04:47,  3.73s/it] 13%|█▎        | 11/86 [00:50<04:45,  3.80s/it] 15%|█▌        | 13/86 [00:54<03:59,  3.28s/it] 17%|█▋        | 15/86 [00:57<03:14,  2.73s/it] 19%|█▊        | 16/86 [01:04<04:07,  3.53s/it] 22%|██▏       | 19/86 [01:16<04:07,  3.69s/it] 23%|██▎       | 20/86 [01:30<05:57,  5.42s/it] 27%|██▋       | 23/86 [01:41<04:50,  4.61s/it] 31%|███▏      | 27/86 [01:46<03:07,  3.17s/it] 36%|███▌      | 31/86 [01:48<01:56,  2.12s/it] 38%|███▊      | 33/86 [01:57<02:22,  2.69s/it] 42%|████▏     | 36/86 [02:08<02:27,  2.94s/it] 43%|████▎     | 37/86 [02:15<02:52,  3.52s/it] 48%|████▊     | 41/86 [02:16<01:32,  2.05s/it] 69%|██████▊   | 59/86 [02:28<00:28,  1.07s/it] 70%|██████▉   | 60/86 [02:30<00:28,  1.08s/it] 71%|███████   | 61/86 [02:53<01:07,  2.69s/it] 76%|███████▌  | 65/86 [03:16<01:16,  3.62s/it] 81%|████████▏ | 70/86 [03:17<00:38,  2.41s/it] 83%|████████▎ | 71/86 [03:18<00:35,  2.35s/it] 85%|████████▍ | 73/86 [03:30<00:39,  3.05s/it] 91%|█████████ | 78/86 [03:33<00:15,  1.99s/it] 92%|█████████▏| 79/86 [03:41<00:18,  2.64s/it] 98%|█████████▊| 84/86 [03:41<00:03,  1.51s/it] 99%|█████████▉| 85/86 [03:49<00:02,  2.18s/it]100%|██████████| 86/86 [03:49<00:00,  2.66s/it]
Found tracks in dataset: 86
Use single GPU: [0]
Patience: 2 Reduce factor: 0.95 Batch size: 2 Grad accum steps: 1 Effective batch size: 2 Optimizer: prodigy
Train for: 1000
Train epoch: 0 Learning rate: 1.0
  0%|          | 0/1000 [00:00<?, ?it/s]  0%|          | 0/1000 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "/project/6002780/kaim/Music-Source-Separation-Training/train_tensorboard.py", line 356, in <module>
    best_metric = train_model(args, base_config, writer=writer)
  File "/project/6002780/kaim/Music-Source-Separation-Training/train_tensorboard.py", line 238, in train_model
    y_ = model(x)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 264, in forward
    sep_output = checkpoint_sequential(self.separator_mask, 2, subband_feature_mask.view(batch_size, nch, self.nband*self.feature_dim, -1))  # B, nband*N, T
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 587, in checkpoint_sequential
    input = checkpoint(
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 574, in forward
    input = functions[j](input)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 128, in forward
    band_output = self.band_rnn(input.view(B*nch*self.nband, self.feature_dim, -1)).view(B*nch, self.nband, -1, T)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 106, in forward
    rnn_output =  self.rnn(self.dropout(self.norm(input)).transpose(1, 2).contiguous())
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 37, in forward
    forward_f_output = self.forward_mamba2(forward_f)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ex_bi_mamba2.py", line 82, in forward
    y = self.ssd(x * dt.unsqueeze(-1),
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ex_bi_mamba2.py", line 112, in ssd
    x = x.reshape(x.shape[0], x.shape[1] // chunk_size, chunk_size, x.shape[2], x.shape[3], )
RuntimeError: shape '[228, 4, 64, 8, 64]' is invalid for input of size 30234624
