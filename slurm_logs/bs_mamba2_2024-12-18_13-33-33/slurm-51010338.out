Running training script for model: bs_mamba2
training without optuna now
Exception during load Mamba2 modules: No module named 'mamba_ssm'
Load local torch implementation!
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 23, 23, 23, 23, 23, 23, 23, 23, 46, 46, 46, 46, 46, 46, 46, 46, 92, 92, 121]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 23, 23, 23, 23, 23, 23, 23, 23, 46, 46, 46, 46, 46, 46, 46, 46, 92, 92, 121]
Instruments: ['drums', 'bass', 'other', 'vocals']
Metrics for training: ['sdr']. Metric for scheduler: sdr
Use augmentation for training
Dataset type: 1 Processes to use: 8 
Collecting metadata for ['../data/MUSDB18HQ/train']
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:21<30:02, 21.20s/it]  2%|▏         | 2/86 [00:28<18:00, 12.86s/it] 10%|█         | 9/86 [00:35<03:31,  2.75s/it] 12%|█▏        | 10/86 [00:40<03:52,  3.06s/it] 13%|█▎        | 11/86 [00:49<05:14,  4.19s/it] 17%|█▋        | 15/86 [00:56<03:27,  2.92s/it] 19%|█▊        | 16/86 [01:03<04:05,  3.50s/it] 22%|██▏       | 19/86 [01:14<03:59,  3.58s/it] 23%|██▎       | 20/86 [01:16<03:46,  3.44s/it] 26%|██▌       | 22/86 [01:23<03:33,  3.34s/it] 27%|██▋       | 23/86 [01:40<06:10,  5.88s/it] 31%|███▏      | 27/86 [01:45<03:28,  3.53s/it] 36%|███▌      | 31/86 [01:47<02:01,  2.20s/it] 38%|███▊      | 33/86 [02:00<02:48,  3.19s/it] 43%|████▎     | 37/86 [02:02<01:47,  2.19s/it] 44%|████▍     | 38/86 [02:06<01:56,  2.43s/it] 45%|████▌     | 39/86 [02:13<02:27,  3.14s/it] 48%|████▊     | 41/86 [02:17<02:00,  2.68s/it] 69%|██████▊   | 59/86 [02:25<00:24,  1.10it/s] 70%|██████▉   | 60/86 [02:29<00:28,  1.09s/it] 71%|███████   | 61/86 [02:44<00:55,  2.22s/it] 73%|███████▎  | 63/86 [02:51<00:54,  2.37s/it] 76%|███████▌  | 65/86 [03:01<01:01,  2.95s/it] 77%|███████▋  | 66/86 [03:05<01:01,  3.09s/it] 81%|████████▏ | 70/86 [03:07<00:31,  1.99s/it] 85%|████████▍ | 73/86 [03:28<00:46,  3.57s/it] 88%|████████▊ | 76/86 [03:36<00:32,  3.26s/it] 98%|█████████▊| 84/86 [03:41<00:03,  1.87s/it] 99%|█████████▉| 85/86 [03:46<00:02,  2.17s/it]100%|██████████| 86/86 [03:46<00:00,  2.64s/it]
Found tracks in dataset: 86
Use single GPU: [0]
Patience: 2 Reduce factor: 0.95 Batch size: 2 Grad accum steps: 1 Effective batch size: 2 Optimizer: prodigy
Train for: 1000
Train epoch: 0 Learning rate: 1.0
  0%|          | 0/1000 [00:00<?, ?it/s]  0%|          | 0/1000 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/project/6002780/kaim/Music-Source-Separation-Training/train_tensorboard.py", line 356, in <module>
    best_metric = train_model(args, base_config, writer=writer)
  File "/project/6002780/kaim/Music-Source-Separation-Training/train_tensorboard.py", line 238, in train_model
    y_ = model(x)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 264, in forward
    sep_output = checkpoint_sequential(self.separator_mask, 2, subband_feature_mask.view(batch_size, nch, self.nband*self.feature_dim, -1))  # B, nband*N, T
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 587, in checkpoint_sequential
    input = checkpoint(
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 574, in forward
    input = functions[j](input)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 128, in forward
    band_output = self.band_rnn(input.view(B*nch*self.nband, self.feature_dim, -1)).view(B*nch, self.nband, -1, T)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 106, in forward
    rnn_output =  self.rnn(self.dropout(self.norm(input)).transpose(1, 2).contiguous())
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ts_bs_mamba2.py", line 37, in forward
    forward_f_output = self.forward_mamba2(forward_f)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/separation_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ex_bi_mamba2.py", line 82, in forward
    y = self.ssd(x * dt.unsqueeze(-1),
  File "/project/6002780/kaim/Music-Source-Separation-Training/models/ex_bi_mamba2.py", line 112, in ssd
    x = x.reshape(x.shape[0], x.shape[1] // chunk_size, chunk_size, x.shape[2], x.shape[3], )
RuntimeError: shape '[228, 4, 64, 8, 64]' is invalid for input of size 30234624
