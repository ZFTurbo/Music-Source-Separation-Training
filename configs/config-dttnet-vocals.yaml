audio:
  chunk_size: 261120     # hop_length * (dim_t - 1) => 1024 * (256 - 1)
  sample_rate: 44100
  num_channels: 2
  min_mean_abs: 0.000

model:
  target_name: vocals    # currently supports only single target training
  audio_ch: 2
  overlap: 768
  n_fft: 6144
  hop_length: 1024
  dim_f: 2048
  dim_t: 256
  num_blocks: 4
  g: 32
  l: 3
  k: 3
  bn: 8
  bias: false
  bn_norm: 'BN'
  block_type: "TFC_TDF_Res2"
  bandsequence:
    rnn_type: 'LSTM'
    bidirectional: true
    num_layers: 4
    n_heads: 2

training:
  batch_size: 4
  gradient_accumulation_steps: 1
  grad_clip: 0.0
  instruments:
  - vocals
  - other
  lr: 0.0002
  patience: 2
  reduce_factor: 0.95
  target_instrument: vocals
  num_epochs: 1000
  num_steps: 1000
  q: 0.95
  coarse_loss_clip: false
  ema_momentum: 0.999
  optimizer: adamw
  other_fix: false # it's needed for checking on multisong dataset if other is actually instrumental
  use_amp: true

inference:
  batch_size: 4
  num_overlap: 4
  normalize: false